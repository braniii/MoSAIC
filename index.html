<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>mosaic API documentation</title>
<meta name="description" content="&lt;div align=&#34;center&#34;&gt;
&lt;img class=&#34;darkmode&#34; style=&#34;width: 400px;&#34; …" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/atom-one-dark.min.css" rel="stylesheet">
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#282c34;-webkit-border-radius:6px;-moz-border-radius:6px;border-radius:6px;margin:1em 0;padding:1ex}#lunr-search{width:100%;font-size:1em;padding:6px 9px 5px 9px;border:1px solid silver;margin-top:10px;-webkit-border-radius:6px;-moz-border-radius:6px;border-radius:6px}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em;-webkit-border-radius:6px;-moz-border-radius:6px;border-radius:6px}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML'></script>
<style>.homelink{display:block;font-size:2em;font-weight:bold;color:#555;padding-bottom:.5em;border-bottom:1px solid silver}.homelink:hover{color:inherit}.homelink img{max-width:100%;max-height:5em;margin:auto;margin-bottom:.3em}.darkmode{display:none !important}</style>
<link rel="icon" href="icon.png">
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>mosaic</code></h1>
</header>
<section id="section-intro">
<div align="center">
<img class="darkmode" style="width: 400px;" src="https://github.com/moldyn/MoSAIC/blob/main/docs/logo_large_dark.svg?raw=true#gh-dark-mode-only" />
<img class="lightmode" style="width: 400px;" src="https://github.com/moldyn/MoSAIC/blob/main/docs/logo_large_light.svg?raw=true#gh-light-mode-only" />
<p>
<a href="https://github.com/wemake-services/wemake-python-styleguide" alt="wemake-python-styleguide" >
<img src="https://img.shields.io/badge/style-wemake-000000.svg" /></a>
<a href="https://beartype.rtfd.io" alt="bear-ified" >
<img src="https://raw.githubusercontent.com/beartype/beartype-assets/main/badge/bear-ified.svg" /></a>
<a href="https://pypi.org/project/mosaic-clustering" alt="PyPI" >
<img src="https://img.shields.io/pypi/v/mosaic-clustering" /></a>
<a href="https://pepy.tech/project/mosaic-clustering" alt="Downloads" >
<img src="https://pepy.tech/badge/mosaic-clustering" /></a>
<a href="https://github.com/moldyn/MoSAIC/actions/workflows/pytest.yml" alt="GitHub Workflow Status">
<img src="https://img.shields.io/github/workflow/status/moldyn/MoSAIC/Pytest"></a>
<a href="https://img.shields.io/pypi/pyversions/mosaic-clustering" alt="PyPI - Python Version">
<img src="https://img.shields.io/pypi/pyversions/mosaic-clustering" /></a>
<a href="https://moldyn.github.io/MoSAIC" alt="Docs" >
<img src="https://img.shields.io/badge/pdoc3-Documentation-brightgreen" /></a>
<a href="#" alt="doi">
<img src="https://img.shields.io/badge/doi-submitted-blue" /></a>
<a href="https://arxiv.org/abs/2204.02770" alt="arXiv">
<img src="https://img.shields.io/badge/arXiv-2204.02770-red" /></a>
<a href="https://github.com/moldyn/MoSAIC/blob/main/LICENSE" alt="License" >
<img src="https://img.shields.io/github/license/moldyn/MoSAIC" /></a>
</p>
<p>
<a href="https://moldyn.github.io/MoSAIC">Docs</a> •
<a href="#features">Features</a> •
<a href="#installation">Installation</a> •
<a href="#usage">Usage</a>
</p>
</div>
<h1 id="molecular-systems-automated-identification-of-cooperativity">Molecular Systems Automated Identification of Cooperativity</h1>
<p>MoSAIC is a new method for correlation analysis which automatically detects
collective motion in MD simulation data, identifies uncorrelated features
as noise and hence provides a detailed picture of the key coordinates driving a
conformational change in a biomolecular system. It is based on the Leiden community
detection algorithm which is used to bring a correlation matrix in a
block-diagonal form.</p>
<p>The method was published in:</p>
<blockquote>
<p>G. Diez, D. Nagel, and G. Stock,
<em>Correlation-based feature selection to identify functional dynamcis
in proteins</em>,
<a href="https://arxiv.org/abs/2204.02770">arXiv:2204.02770</a></p>
</blockquote>
<p>We kindly ask you to cite this article in case you use this software package for
published works.</p>
<h2 id="features">Features</h2>
<ul>
<li>Intuitive usage via <a href="#module---inside-a-python-script">module</a> and via <a href="#ci---usage-directly-from-the-command-line">CI</a></li>
<li>Sklearn-style API for fast integration into your Python workflow</li>
<li>No magic, only a
single parameter</li>
<li>Extensive <a href="https://moldyn.github.io/feature_selection">documentation</a> and
detailed discussion in publication</li>
</ul>
<h2 id="installation">Installation</h2>
<p>So far the package is only published to <a href="https://pypi.org">PyPI</a>. Soon, it will
be added <a href="https://conda-forge.org/">conda-forge</a>, as well. To install it within a python environment simple call:</p>
<pre><code class="language-bash">python3 -m pip install --upgrade moldyn-mosaic
</code></pre>
<p>or for the latest dev version</p>
<pre><code class="language-bash"># via ssh key
python3 -m pip install git+ssh://git@github.com/moldyn/MoSAIC.git

# or via password-based login
python3 -m pip install git+https://github.com/moldyn/MoSAIC.git
</code></pre>
<h3 id="shell-completion">Shell Completion</h3>
<p>Using the <code>bash</code>, <code>zsh</code> or <code>fish</code> shell click provides an easy way to
provide shell completion, checkout the
<a href="https://click.palletsprojects.com/en/8.0.x/shell-completion">docs</a>.
In the case of bash you need to add following line to your <code>~/.bashrc</code></p>
<pre><code class="language-bash">eval &quot;$(_MOSAIC_COMPLETE=bash_source MoSAIC)&quot;
</code></pre>
<h2 id="usage">Usage</h2>
<p>In general one can call the module directly by its entry point <code>$ MoSAIC</code>
or by calling the module <code>$ python -m mosaic</code>. The latter method is
preferred to ensure using the desired python environment. For enabling
the shell completion, the entry point needs to be used.</p>
<h3 id="ci-usage-directly-from-the-command-line">CI - Usage Directly from the Command Line</h3>
<p>The module brings a rich CI using <a href="https://click.palletsprojects.com">click</a>.
Each module and submodule contains a detailed help, which can be accessed by</p>
<pre><code class="language-bash">$ python -m mosaic
Usage: python -m mosaic [OPTIONS] COMMAND [ARGS]...

  MoSAIC motion v0.1.0

  Molecular systems automated identification of collective motion, is
  a correlation based feature selection framework for MD data.
  Copyright (c) 2022, Georg Diez and Daniel Nagel

Options:
  --help  Show this message and exit.

Commands:
  clustering  Clustering similarity matrix of coordinates.
  similarity  Creating similarity matrix of coordinates.
  umap        Embedd similarity matrix with UMAP.
</code></pre>
<p>For more details on the submodule one needs to specify one of the three
commands.</p>
<p>A simple workflow example for clustering the input file <code>input_file</code> using
correlation and Leiden with CPM and the default resolution parameter:</p>
<pre><code class="language-bash"># creating correlation matrix
$ python -m mosaic similarity -i input_file -o output_similarity -metric correlation -v

MoSAIC SIMILARITY
~~~ Initialize similarity class
~~~ Load file input_file
~~~ Fit input
~~~ Store similarity matrix in output_similarity

# clustering with CPM and default resolution parameter
# the latter needs to be fine-tuned to each matrix
$ python -m mosaic clustering -i output_similarity -o output_clustering --plot -v

MoSAIC CLUSTERING
~~~ Initialize clustering class
~~~ Load file output_similarity
~~~ Fit input
~~~ Store output
~~~ Plot matrix
</code></pre>
<p>This will generate the similarity matrix stored in <code>output_similarity</code>,
the plotted result in <code>output_clustering.matrix.pdf</code>, the raw data of
the matrix in <code>output_clustering.matrix</code> and a file containing in each
row the indices of a cluster.</p>
<h3 id="module-inside-a-python-script">Module - Inside a Python Script</h3>
<pre><code class="language-python">import mosaic

# Load file
# X is np.ndarray of shape (n_samples, n_features)

sim = mosaic.Similarity(
    metric='correlation',  # or 'NMI', 'GY', 'JSD'
)
sim.fit(X)


# Cluster matrix
clust = mosaic.Clustering(
    mode='CPM',  # or 'modularity
)
clust.fit(sim.matrix_)

clusters = clust.clusters_
clusterd_X = clust.matrix_
...
</code></pre>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
&#34;&#34;&#34;.. include:: ../../README.md&#34;&#34;&#34;
__all__ = [&#39;Clustering&#39;, &#39;load_clusters&#39;, &#39;Similarity&#39;, &#39;UMAPSimilarity&#39;]

from .clustering import Clustering
from .similarity import Similarity
from .tools import load_clusters
from .umap_similarity import UMAPSimilarity


__version__ = &#39;0.2.0&#39;</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="mosaic.clustering" href="clustering.html">mosaic.clustering</a></code></dt>
<dd>
<div class="desc"><p>Class for clustering the correlation matrices …</p></div>
</dd>
<dt><code class="name"><a title="mosaic.similarity" href="similarity.html">mosaic.similarity</a></code></dt>
<dd>
<div class="desc"><p>Class for estimating correlation matrices …</p></div>
</dd>
<dt><code class="name"><a title="mosaic.tools" href="tools.html">mosaic.tools</a></code></dt>
<dd>
<div class="desc"><p>Class with helper functions …</p></div>
</dd>
<dt><code class="name"><a title="mosaic.umap_similarity" href="umap_similarity.html">mosaic.umap_similarity</a></code></dt>
<dd>
<div class="desc"><p>Class for embedding correlation matrix with UMAP …</p></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="mosaic.load_clusters"><code class="name flex">
<span>def <span class="ident">load_clusters</span></span>(<span>filename)</span>
</code></dt>
<dd>
<div class="desc"><p>Load clusters stored from cli.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Filename of cluster file.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype
def load_clusters(filename: str) -&gt; Object1DArray:
    &#34;&#34;&#34;Load clusters stored from cli.

    Parameters
    ----------
    filename : str
        Filename of cluster file.

    &#34;&#34;&#34;
    clusters_list =  [
        np.array(
            cluster.split()
        ).astype(int).tolist()
        for cluster in np.loadtxt(filename, delimiter=&#39;\n&#39;, dtype=str)
    ]

    # In case of clusters of same length, numpy casted it as a 2D array.
    # To ensure that the result is an numpy array of list, we need to
    # create an empty list, adding the values in the second step
    clusters: Object1DArray = np.empty(len(clusters_list), dtype=object)
    clusters[:] = clusters_list
    return clusters</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="mosaic.Clustering"><code class="flex name class">
<span>class <span class="ident">Clustering</span></span>
<span>(</span><span>*, mode='CPM', weighted=True, n_neighbors=None, resolution_parameter=None, n_clusters=None, seed=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Class for clustering a correlation matrix.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code>, default=<code>'CPM'</code></dt>
<dd>the mode which determines the quality function optimized by the Leiden
algorithm ('CPM', or 'modularity') or linkage clustering.
- 'CPM': will use the constant Potts model on the full, weighted graph
- 'modularity': will use modularity on a knn-graph
- 'linkage': will use complete-linkage clustering
- 'kmedoids': will use $k$-medoids clustering</dd>
<dt><strong><code>weighted</code></strong> :&ensp;<code>bool</code>, default=<code>True,</code></dt>
<dd>If True, the underlying graph has weighted edges. Otherwise, the graph
is constructed using the adjacency matrix.</dd>
<dt><strong><code>n_neighbors</code></strong> :&ensp;<code>int</code>, default=<code>None,</code></dt>
<dd>This parameter specifies if the whole matrix is used, or an knn-graph.
The default depends on the <code>mode</code>
- 'CPM': <code>None</code> uses full graph, and
- 'modularity': <code>None</code> uses square root of the number of features.</dd>
<dt><strong><code>resolution_parameter</code></strong> :&ensp;<code>float</code>, default=<code>None,</code></dt>
<dd>Required for mode 'CPM' and 'linkage'. If None, the resolution
parameter will be set to the third quartile of <code>X</code> for
<code>n_neighbors=None</code> and else to the mean value of the knn graph.</dd>
<dt><strong><code>n_clusters</code></strong> :&ensp;<code>int</code>, default=<code>None,</code></dt>
<dd>Required for 'kmedoids'. The number of clusters to form.</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code>, default=<code>None,</code></dt>
<dd>Use an integer to make the randomness of Leidenalg deterministic. By
default uses a random seed if nothing is specified.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>clusters_</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_clusters, )</code></dt>
<dd>The result of the clustering process. A list of arrays, each
containing all indices (features) for each cluster.</dd>
<dt><strong><code>labels_</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_features, )</code></dt>
<dd>Labels of each feature.</dd>
<dt><strong><code>matrix_</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_features, n_features)</code></dt>
<dd>Permuted matrix according to the found clusters.</dd>
<dt><strong><code>ticks_</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_clusters, )</code></dt>
<dd>Get cumulative indices where new cluster starts in <code>matrix_</code>.</dd>
<dt><strong><code>permutation_</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_features, )</code></dt>
<dd>Permutation of the input features (corresponds to flattened
<code>clusters_</code>).</dd>
<dt><strong><code>n_neighbors_</code></strong> :&ensp;<code>int</code></dt>
<dd>Only avaiable when using knn graph. Indicates the number of nearest
neighbors used for constructin the knn-graph.</dd>
<dt><strong><code>resolution_param_</code></strong> :&ensp;<code>float</code></dt>
<dd>Only for mode 'CPM' and 'linkage'. Indicates the resolution parameter
used for the CPM based Leiden clustering.</dd>
<dt><strong><code>linkage_matrix_</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_clusters - 1, 4)</code></dt>
<dd>Only for mode 'linkage'. Holds hierarchicak clustering encoded as a
linkage matrix, see
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html">scipy:spatial.distance.linkage</a>.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import mosaic
&gt;&gt;&gt; mat = np.array([[1.0, 0.1, 0.9], [0.1, 1.0, 0.1], [0.9, 0.1, 1.0]])
&gt;&gt;&gt; clust = mosaic.Clustering()
&gt;&gt;&gt; clust.fit(mat)
&gt;&gt;&gt; clust.matrix_
array([[1. , 0.9, 0.1],
       [0.9, 1. , 0.1],
       [0.1, 0.1, 1. ]])
&gt;&gt;&gt; clust.clusters_
array([list([2, 0]), list([1])], dtype=object)
</code></pre>
<p>Initialize Clustering class.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Clustering:
    r&#34;&#34;&#34;Class for clustering a correlation matrix.

    Parameters
    ----------
    mode : str, default=&#39;CPM&#39;
        the mode which determines the quality function optimized by the Leiden
        algorithm (&#39;CPM&#39;, or &#39;modularity&#39;) or linkage clustering.
        - &#39;CPM&#39;: will use the constant Potts model on the full, weighted graph
        - &#39;modularity&#39;: will use modularity on a knn-graph
        - &#39;linkage&#39;: will use complete-linkage clustering
        - &#39;kmedoids&#39;: will use $k$-medoids clustering

    weighted : bool, default=True,
        If True, the underlying graph has weighted edges. Otherwise, the graph
        is constructed using the adjacency matrix.

    n_neighbors : int, default=None,
        This parameter specifies if the whole matrix is used, or an knn-graph.
        The default depends on the `mode`
        - &#39;CPM&#39;: `None` uses full graph, and
        - &#39;modularity&#39;: `None` uses square root of the number of features.

    resolution_parameter : float, default=None,
        Required for mode &#39;CPM&#39; and &#39;linkage&#39;. If None, the resolution
        parameter will be set to the third quartile of `X` for
        `n_neighbors=None` and else to the mean value of the knn graph.

    n_clusters : int, default=None,
        Required for &#39;kmedoids&#39;. The number of clusters to form.

    seed : int, default=None,
        Use an integer to make the randomness of Leidenalg deterministic. By
        default uses a random seed if nothing is specified.

    Attributes
    ----------
    clusters_ : ndarray of shape (n_clusters, )
        The result of the clustering process. A list of arrays, each
        containing all indices (features) for each cluster.

    labels_ : ndarray of shape (n_features, )
        Labels of each feature.

    matrix_ : ndarray of shape (n_features, n_features)
        Permuted matrix according to the found clusters.

    ticks_ : ndarray of shape (n_clusters, )
        Get cumulative indices where new cluster starts in `matrix_`.

    permutation_ : ndarray of shape (n_features, )
        Permutation of the input features (corresponds to flattened
        `clusters_`).

    n_neighbors_ : int
        Only avaiable when using knn graph. Indicates the number of nearest
        neighbors used for constructin the knn-graph.

    resolution_param_ : float
        Only for mode &#39;CPM&#39; and &#39;linkage&#39;. Indicates the resolution parameter
        used for the CPM based Leiden clustering.

    linkage_matrix_ : ndarray of shape (n_clusters - 1, 4)
        Only for mode &#39;linkage&#39;. Holds hierarchicak clustering encoded as a
        linkage matrix, see
        [scipy:spatial.distance.linkage](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html).

    Examples
    --------
    &gt;&gt;&gt; import mosaic
    &gt;&gt;&gt; mat = np.array([[1.0, 0.1, 0.9], [0.1, 1.0, 0.1], [0.9, 0.1, 1.0]])
    &gt;&gt;&gt; clust = mosaic.Clustering()
    &gt;&gt;&gt; clust.fit(mat)
    &gt;&gt;&gt; clust.matrix_
    array([[1. , 0.9, 0.1],
           [0.9, 1. , 0.1],
           [0.1, 0.1, 1. ]])
    &gt;&gt;&gt; clust.clusters_
    array([list([2, 0]), list([1])], dtype=object)

    &#34;&#34;&#34;

    @beartype
    def __init__(
        self,
        *,
        mode: ClusteringModeString = &#39;CPM&#39;,
        weighted: bool = True,
        n_neighbors: Optional[PositiveInt] = None,
        resolution_parameter: Optional[NumInRange0to1] = None,
        n_clusters: Optional[PositiveInt] = None,
        seed: Optional[int] = None,
    ) -&gt; None:
        &#34;&#34;&#34;Initialize Clustering class.&#34;&#34;&#34;
        self._mode: ClusteringModeString = mode
        self._weighted: bool = weighted
        self._neighbors: Optional[PositiveInt] = n_neighbors
        self._seed: Optional[int] = seed
        self._n_clusters: Optional[PositiveInt] = n_clusters

        if mode in {&#39;linkage&#39;, &#39;kmedoids&#39;} and self._neighbors is not None:
            raise NotImplementedError(
                f&#34;mode=&#39;{mode}&#39; does not support knn-graphs.&#34;,
            )

        if mode == &#39;kmedoids&#39; and self._n_clusters is None:
            raise NotImplementedError(
                &#34;mode=&#39;kemdoids&#39; needs parameter &#39;n_clusters&#39;&#34;,
            )
        if self._n_clusters is not None:
            raise NotImplementedError(
                f&#34;mode=&#39;{mode}&#39; does not support the usage of &#39;n_clusters&#39;&#34;,
            )

        if mode in {&#39;CPM&#39;, &#39;linkage&#39;}:
            self._resolution_parameter: Optional[NumInRange0to1] = (
                resolution_parameter
            )
            if not weighted:
                raise NotImplementedError(
                    f&#34;mode=&#39;{mode}&#39; does not support weighted=False&#34;,
                )
        elif resolution_parameter is not None:
            raise NotImplementedError(
                f&#34;mode=&#39;{mode}&#39; does not support the usage of the &#34;
                &#39;resolution_parameter&#39;,
            )

    @beartype
    def fit(self, X: SimilarityMatrix, y: Optional[np.ndarray] = None) -&gt; None:
        &#34;&#34;&#34;Clusters the correlation matrix by Leiden clustering on a graph.

        Parameters
        ----------
        X : ndarray of shape (n_features, n_features)
            Matrix containing the correlation metric which is clustered. The
            values should go from [0, 1] where 1 means completely correlated
            and 0 no correlation.

        y : Ignored
            Not used, present for scikit API consistency by convention.

        &#34;&#34;&#34;
        self._reset()

        # prepare matric for graph construction
        mat: FloatMatrix
        if self._mode in {&#39;linkage&#39;, &#39;kmedoids&#39;}:
            mat = np.copy(X)
        elif self._mode == &#39;CPM&#39; and self._neighbors is None:
            mat = np.copy(X)
        else:
            mat = self._construct_knn_mat(X)
        # mask diagonal and zero elements
        mat[mat == 0] = np.nan
        mat[np.diag_indices_from(mat)] = np.nan

        if self._mode in {&#39;CPM&#39;, &#39;linkage&#39;}:
            if self._resolution_parameter is None:
                if self._neighbors is None:
                    third_quartile = 0.75
                    self._resolution_parameter = np.nanquantile(
                        mat, third_quartile,
                    )
                else:
                    self._resolution_parameter = np.nanmean(mat)

            self.resolution_param_: NumInRange0to1 = (
                self._resolution_parameter
            )

        # create graph
        mat[np.isnan(mat)] = 0

        clusters: Object1DArray
        if self._mode == &#39;linkage&#39;:
            clusters = self._clustering_linkage(mat)
        elif self._mode == &#39;kmedoids&#39;:
            clusters = self._clustering_kmedoids(mat)
        else:  # _mode in {&#39;CPM&#39;, &#39;modularity&#39;}
            graph: ig.Graph = ig.Graph.Weighted_Adjacency(
                list(mat.astype(np.float64)), loops=False,
            )
            clusters = self._clustering_leiden(graph)

        self.clusters_: Object1DArray = _sort_clusters(clusters, X)
        self.permutation_: Index1DArray = np.hstack(self.clusters_)
        self.matrix_: Float2DArray = np.copy(X)[
            np.ix_(self.permutation_, self.permutation_)
        ]
        self.ticks_: Index1DArray = np.cumsum(
            [len(cluster) for cluster in self.clusters_],
        )
        labels: Index1DArray = np.empty_like(self.permutation_)
        for idx, cluster in enumerate(self.clusters_):
            labels[cluster] = idx
        self.labels_: Index1DArray = labels

    @beartype
    def _reset(self) -&gt; None:
        &#34;&#34;&#34;Reset internal data-dependent state of correlation.&#34;&#34;&#34;
        if hasattr(self, &#39;clusters_&#39;):  # noqa: WPS421
            del self.clusters_  # noqa: WPS420
            del self.labels_  # noqa: WPS420
            del self.ticks_  # noqa: WPS420
            del self.permutation_  # noqa: WPS420
            del self.matrix_  # noqa: WPS420

        if hasattr(self, &#39;linkage_matrix_&#39;):  # noqa: WPS421
            del self.linkage_matrix_  # noqa: WPS420
        if hasattr(self, &#39;n_neighbors_&#39;):  # noqa: WPS421
            del self.n_neighbors_  # noqa: WPS420
        if hasattr(self, &#39;resolution_param_&#39;):  # noqa: WPS421
            del self.resolution_param_  # noqa: WPS420
        if hasattr(self, &#39;n_clusters_&#39;):  # noqa: WPS421
            del self.n_clusters_  # noqa: WPS420

    @beartype
    def _construct_knn_mat(self, matrix: FloatMatrix) -&gt; FloatMatrix:
        &#34;&#34;&#34;Construct the knn matrix.&#34;&#34;&#34;
        if self._neighbors is None:
            n_features = len(matrix)
            self._neighbors = np.ceil(np.sqrt(n_features)).astype(int)
        elif self._neighbors &gt;= len(matrix):
            raise ValueError(
                &#39;The number of nearest neighbors must be smaller than the &#39;
                &#39;number of features.&#39;,
            )
        self.n_neighbors_: PositiveInt = self._neighbors

        neigh = NearestNeighbors(
            n_neighbors=self._neighbors,
            metric=&#39;precomputed&#39;,
        )
        neigh.fit(1 - matrix)
        if self._weighted:
            dist_mat = neigh.kneighbors_graph(mode=&#39;distance&#39;).toarray()
            dist_mat[dist_mat == 0] = 1
            return 1 - dist_mat
        return neigh.kneighbors_graph(mode=&#39;connectivity&#39;).toarray()

    @beartype
    def _setup_leiden_kwargs(self, graph: ig.Graph) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;Set up the parameters for the Leiden clustering.&#34;&#34;&#34;
        kwargs_leiden = {&#39;n_iterations&#39;: -1}
        if self._mode == &#39;CPM&#39;:
            kwargs_leiden[&#39;partition_type&#39;] = la.CPMVertexPartition
            kwargs_leiden[
                &#39;resolution_parameter&#39;
            ] = self.resolution_param_
        else:
            kwargs_leiden[
                &#39;partition_type&#39;
            ] = la.ModularityVertexPartition
        if self._weighted:
            kwargs_leiden[&#39;weights&#39;] = graph.es[&#39;weight&#39;]

        kwargs_leiden[&#39;seed&#39;] = self._seed

        return kwargs_leiden

    @beartype
    def _clustering_leiden(self, graph: ig.Graph) -&gt; Object1DArray:
        &#34;&#34;&#34;Perform the Leiden clustering on the graph.&#34;&#34;&#34;
        clusters = la.find_partition(
            graph, **self._setup_leiden_kwargs(graph),
        )
        # In case of clusters of same length, numpy casted it as a 2D array.
        # To ensure that the result is an numpy array of list, we need to
        # create an empty list, adding the values in the second step
        cluster_list: Object1DArray = np.empty(len(clusters), dtype=object)
        cluster_list[:] = clusters  # noqa: WPS362
        return cluster_list

    @beartype
    def _clustering_linkage(self, matrix: FloatMatrix) -&gt; Object1DArray:
        &#34;&#34;&#34;Perform the linkage clustering.&#34;&#34;&#34;
        matrix[np.diag_indices_from(matrix)] = 1
        linkage_matrix: Float2DArray = linkage(
            squareform(1 - matrix),
            method=&#39;complete&#39;,
            optimal_ordering=True,
        )
        # store linkage tree
        self.linkage_matrix_: Float2DArray = linkage_matrix

        cuttree: Index1DArray = cut_tree(
            linkage_matrix, height=1 - self.resolution_param_,
        ).flatten()

        # In case of clusters of same length, numpy casted it as a 2D array.
        # To ensure that the result is an numpy array of list, we need to
        # create an empty list, adding the values in the second step
        nclusters: int = len(np.unique(cuttree))
        cluster_list: Object1DArray = np.empty(nclusters, dtype=object)
        cluster_list[:] = [  # noqa: WPS362
            np.where(cuttree == cluster)[0].tolist()
            for cluster in np.unique(cuttree)
        ]
        return cluster_list

    @beartype
    def _clustering_kmedoids(self, matrix: FloatMatrix) -&gt; Object1DArray:
        &#34;&#34;&#34;Perform k-medoids clustering.&#34;&#34;&#34;
        kmedoids_kwargs = {
            &#39;metric&#39;: &#39;precomputed&#39;,
            &#39;max_iter&#39;: 100000,
            &#39;method&#39;: &#39;pam&#39;,
        }

        kmedoids = KMedoids(**kmedoids_kwargs, n_clusters=self._n_clusters)
        kmedoids.fit(1 - matrix)
        labels = kmedoids.labels_

        # store number of clusters
        nclusters: int = len(np.unique(labels))
        if nclusters != self._n_clusters:
            raise ValueError(
                f&#39;k-medoids tried to find {self._n_clusters} clusters&#39;
                f&#39;but only {nclusters} found. Please try a different value.&#39;
            )
        self.n_clusters_: Float2DArray = nclusters

        # In case of clusters of same length, numpy casted it as a 2D array.
        # To ensure that the result is an numpy array of list, we need to
        # create an empty list, adding the values in the second step
        cluster_list: Object1DArray = np.empty(nclusters, dtype=object)
        cluster_list[:] = [  # noqa: WPS362
            np.where(labels == label)[0].tolist()
            for label in np.unique(labels)
        ]
        return cluster_list</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="mosaic.Clustering.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Clusters the correlation matrix by Leiden clustering on a graph.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_features, n_features)</code></dt>
<dd>Matrix containing the correlation metric which is clustered. The
values should go from [0, 1] where 1 means completely correlated
and 0 no correlation.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>Ignored</code></dt>
<dd>Not used, present for scikit API consistency by convention.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype
def fit(self, X: SimilarityMatrix, y: Optional[np.ndarray] = None) -&gt; None:
    &#34;&#34;&#34;Clusters the correlation matrix by Leiden clustering on a graph.

    Parameters
    ----------
    X : ndarray of shape (n_features, n_features)
        Matrix containing the correlation metric which is clustered. The
        values should go from [0, 1] where 1 means completely correlated
        and 0 no correlation.

    y : Ignored
        Not used, present for scikit API consistency by convention.

    &#34;&#34;&#34;
    self._reset()

    # prepare matric for graph construction
    mat: FloatMatrix
    if self._mode in {&#39;linkage&#39;, &#39;kmedoids&#39;}:
        mat = np.copy(X)
    elif self._mode == &#39;CPM&#39; and self._neighbors is None:
        mat = np.copy(X)
    else:
        mat = self._construct_knn_mat(X)
    # mask diagonal and zero elements
    mat[mat == 0] = np.nan
    mat[np.diag_indices_from(mat)] = np.nan

    if self._mode in {&#39;CPM&#39;, &#39;linkage&#39;}:
        if self._resolution_parameter is None:
            if self._neighbors is None:
                third_quartile = 0.75
                self._resolution_parameter = np.nanquantile(
                    mat, third_quartile,
                )
            else:
                self._resolution_parameter = np.nanmean(mat)

        self.resolution_param_: NumInRange0to1 = (
            self._resolution_parameter
        )

    # create graph
    mat[np.isnan(mat)] = 0

    clusters: Object1DArray
    if self._mode == &#39;linkage&#39;:
        clusters = self._clustering_linkage(mat)
    elif self._mode == &#39;kmedoids&#39;:
        clusters = self._clustering_kmedoids(mat)
    else:  # _mode in {&#39;CPM&#39;, &#39;modularity&#39;}
        graph: ig.Graph = ig.Graph.Weighted_Adjacency(
            list(mat.astype(np.float64)), loops=False,
        )
        clusters = self._clustering_leiden(graph)

    self.clusters_: Object1DArray = _sort_clusters(clusters, X)
    self.permutation_: Index1DArray = np.hstack(self.clusters_)
    self.matrix_: Float2DArray = np.copy(X)[
        np.ix_(self.permutation_, self.permutation_)
    ]
    self.ticks_: Index1DArray = np.cumsum(
        [len(cluster) for cluster in self.clusters_],
    )
    labels: Index1DArray = np.empty_like(self.permutation_)
    for idx, cluster in enumerate(self.clusters_):
        labels[cluster] = idx
    self.labels_: Index1DArray = labels</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="mosaic.Similarity"><code class="flex name class">
<span>class <span class="ident">Similarity</span></span>
<span>(</span><span>*, metric='correlation', online=False, normalize_method=None, knn_estimator=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Class for calculating the similarity measure.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>metric</code></strong> :&ensp;<code>str</code>, default=<code>'correlation'</code></dt>
<dd>
<p>the correlation metric to use for the feature distance matrix.</p>
<ul>
<li><code>'correlation'</code> will use the absolute value of the Pearson
correlation</li>
<li><code>'NMI'</code> will use the mutual information normalized by joined entropy</li>
<li><code>'GY'</code> uses Gel'fand and Yaglom normalization<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup></li>
<li><code>'JSD'</code> will use the Jensen-Shannon divergence between the joint
probability distribution and the product of the marginal probability
distributions to calculate their dissimilarity</li>
</ul>
<p>Note: <code>'NMI'</code> is supported only with online=False</p>
</dd>
<dt><strong><code>online</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>If True, the input of fit X needs to be a file name and the correlation
is calculated on the fly. Otherwise, an array is assumed as input X.</dd>
<dt><strong><code>normalize_method</code></strong> :&ensp;<code>str</code>, default=<code>'geometric'</code></dt>
<dd>
<p>Only required for metric <code>'NMI'</code>. Determines the normalization factor
for the mutual information:</p>
<ul>
<li><code>'joint'</code> is the joint entropy</li>
<li><code>'max'</code>is the maximum of the individual entropies</li>
<li><code>'arithmetic'</code> is the mean of the individual entropies</li>
<li><code>'geometric'</code> is the square root of the product of the individual
entropies</li>
<li><code>'min'</code> is the minimum of the individual entropies</li>
</ul>
</dd>
<dt><strong><code>knn_estimator</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Can only be set for metric GY. If True, the mutual information
is estimated reliably by a parameter free method based on entropy
estimation from k-nearest neighbors distances<sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup>.
It considerably increases the computational time and is thus
only advisable for relatively small data-sets.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>matrix_</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_features, n_features)</code></dt>
<dd>The correlation-measure-based pairwise distance matrix of the data. It
scales from [0, 1].</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import mosaic
&gt;&gt;&gt; x = np.linspace(0, np.pi, 1000)
&gt;&gt;&gt; data = np.array([np.cos(x), np.cos(x + np.pi / 6)]).T
&gt;&gt;&gt; sim = mosaic.Similarity()
&gt;&gt;&gt; sim.fit(data)
&gt;&gt;&gt; sim.matrix_
array([[1.       , 0.9697832],
       [0.9697832, 1.       ]])
</code></pre>
<h2 id="notes">Notes</h2>
<p>The correlation is defined as
<span><span class="MathJax_Preview">\rho_{X,Y} =
\frac{\langle(X -\mu_X)(Y -\mu_Y)\rangle}{\sigma_X\sigma_Y}</span><script type="math/tex; mode=display">\rho_{X,Y} =
\frac{\langle(X -\mu_X)(Y -\mu_Y)\rangle}{\sigma_X\sigma_Y}</script></span>
where for the online algorithm the Welford algorithm taken from Donald E.
Knuth were used <sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup>.</p>
<p>The Jensen-Shannon divergence is defined as
<span><span class="MathJax_Preview">D_{\text{JS}} = \frac{1}{2} D_{\text{KL}}(p(x,y)||M)
+ \frac{1}{2} D_{\text{KL}}(p(x)p(y)||M)\;,</span><script type="math/tex; mode=display">D_{\text{JS}} = \frac{1}{2} D_{\text{KL}}(p(x,y)||M)
+ \frac{1}{2} D_{\text{KL}}(p(x)p(y)||M)\;,</script></span>
where <span><span class="MathJax_Preview">M = \frac{1}{2} [p(x,y) + p(x)p(y)]</span><script type="math/tex">M = \frac{1}{2} [p(x,y) + p(x)p(y)]</script></span> is an averaged probability
distribution and <span><span class="MathJax_Preview">D_{\text{KL}}</span><script type="math/tex">D_{\text{KL}}</script></span> denotes the Kullback-Leibler divergence.</p>
<p>Initialize Similarity class.</p>
<div class="footnote">
<hr>
<ol>
<li id="fn:1">
<p>Gel'fand, I.M. and Yaglom, A.M. (1957). "Calculation of amount of
information about a random function contained in another such
function".
American Mathematical Society Translations, series 2, 12, pp. 199–246.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>Welford algorithm, generalized to correlation. Taken from:
Donald E. Knuth (1998). "The Art of Computer Programming", volume 2:
Seminumerical Algorithms, 3rd edn., p. 232. Boston: Addison-Wesley.&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p>B.C. Ross, PLoS ONE 9(2) (2014), "Mutual Information between Discrete
and Continuous Data Sets"&#160;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
</ol>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Similarity:  # noqa: WPS214
    r&#34;&#34;&#34;Class for calculating the similarity measure.

    Parameters
    ----------
    metric : str, default=&#39;correlation&#39;
        the correlation metric to use for the feature distance matrix.

        - `&#39;correlation&#39;` will use the absolute value of the Pearson
          correlation
        - `&#39;NMI&#39;` will use the mutual information normalized by joined entropy
        - `&#39;GY&#39;` uses Gel&#39;fand and Yaglom normalization[^1]
        - `&#39;JSD&#39;` will use the Jensen-Shannon divergence between the joint
          probability distribution and the product of the marginal probability
          distributions to calculate their dissimilarity

        Note: `&#39;NMI&#39;` is supported only with online=False

    online : bool, default=False
        If True, the input of fit X needs to be a file name and the correlation
        is calculated on the fly. Otherwise, an array is assumed as input X.

    normalize_method : str, default=&#39;geometric&#39;
        Only required for metric `&#39;NMI&#39;`. Determines the normalization factor
        for the mutual information:

        - `&#39;joint&#39;` is the joint entropy
        - `&#39;max&#39;`is the maximum of the individual entropies
        - `&#39;arithmetic&#39;` is the mean of the individual entropies
        - `&#39;geometric&#39;` is the square root of the product of the individual
          entropies
        - `&#39;min&#39;` is the minimum of the individual entropies

    knn_estimator : bool, default=False
        Can only be set for metric GY. If True, the mutual information
        is estimated reliably by a parameter free method based on entropy
        estimation from k-nearest neighbors distances[^3].
        It considerably increases the computational time and is thus
        only advisable for relatively small data-sets.

    Attributes
    ----------
    matrix_ : ndarray of shape (n_features, n_features)
        The correlation-measure-based pairwise distance matrix of the data. It
        scales from [0, 1].

    Examples
    --------
    &gt;&gt;&gt; import mosaic
    &gt;&gt;&gt; x = np.linspace(0, np.pi, 1000)
    &gt;&gt;&gt; data = np.array([np.cos(x), np.cos(x + np.pi / 6)]).T
    &gt;&gt;&gt; sim = mosaic.Similarity()
    &gt;&gt;&gt; sim.fit(data)
    &gt;&gt;&gt; sim.matrix_
    array([[1.       , 0.9697832],
           [0.9697832, 1.       ]])


    Notes
    -----
    The correlation is defined as
    $$\rho_{X,Y} =
    \frac{\langle(X -\mu_X)(Y -\mu_Y)\rangle}{\sigma_X\sigma_Y}$$
    where for the online algorithm the Welford algorithm taken from Donald E.
    Knuth were used [^2].

    [^1]: Gel&#39;fand, I.M. and Yaglom, A.M. (1957). &#34;Calculation of amount of
        information about a random function contained in another such
        function&#34;.
        American Mathematical Society Translations, series 2, 12, pp. 199–246.

    [^2]: Welford algorithm, generalized to correlation. Taken from:
        Donald E. Knuth (1998). &#34;The Art of Computer Programming&#34;, volume 2:
        Seminumerical Algorithms, 3rd edn., p. 232. Boston: Addison-Wesley.

    [^3]: B.C. Ross, PLoS ONE 9(2) (2014), &#34;Mutual Information between Discrete
        and Continuous Data Sets&#34;

    The Jensen-Shannon divergence is defined as
    $$D_{\text{JS}} = \frac{1}{2} D_{\text{KL}}(p(x,y)||M)
    + \frac{1}{2} D_{\text{KL}}(p(x)p(y)||M)\;,$$
    where \(M = \frac{1}{2} [p(x,y) + p(x)p(y)]\) is an averaged probability
    distribution and \(D_{\text{KL}}\) denotes the Kullback-Leibler divergence.

    &#34;&#34;&#34;

    _dtype: np.dtype = np.float64
    _default_normalize_method: str = &#39;geometric&#39;

    @beartype
    def __init__(
        self,
        *,
        metric: MetricString = &#39;correlation&#39;,
        online: bool = False,
        normalize_method: Optional[NormString] = None,
        knn_estimator: bool = False,
    ):
        &#34;&#34;&#34;Initialize Similarity class.&#34;&#34;&#34;
        self._metric: MetricString = metric
        self._online: bool = online
        self._knn_estimate: bool = knn_estimator
        if self._metric == &#39;NMI&#39;:
            if normalize_method is None:
                normalize_method = self._default_normalize_method
            self._normalize_method: NormString = normalize_method
        elif normalize_method is not None:
            raise NotImplementedError(
                &#39;Normalize methods are only supported with metric=&#34;NMI&#34;&#39;,
            )
        if self._metric != &#39;GY&#39; and self._knn_estimate:
            raise NotImplementedError(
                (
                    &#39;The mutual information estimate based on k-nearest&#39;
                    &#39;neighbors distances is only supported with metric=&#34;GY&#34;&#39;
                )
            )

    @singledispatchmethod
    @beartype
    def fit(
        self,
        X: Union[FloatMax2DArray, str],
        y: Optional[ArrayLikeFloat] = None,
    ) -&gt; None:
        &#34;&#34;&#34;Compute the correlation/nmi distance matrix.

        Parameters
        ----------
        X : ndarray of shape (n_samples, n_features) or str if online=True
            Training data.

        y : Ignored
            Not used, present for scikit API consistency by convention.

        &#34;&#34;&#34;
        raise NotImplementedError(&#39;Fatal error, this should never be reached.&#39;)

    @fit.register
    def _(self, X: np.ndarray, y=None) -&gt; None:
        &#34;&#34;&#34;Dispatched for online=False with matrix input.&#34;&#34;&#34;
        self._reset()

        corr: np.ndarray
        matrix_: np.ndarray
        # parse data
        if self._online:
            raise TypeError(&#39;Using online=True requires X:str&#39;)
        if X.ndim == 1:
            raise ValueError(
                &#39;Reshape your data either using array.reshape(-1, 1) if your &#39;
                &#39;data has a single feature or array.reshape(1, -1) if it &#39;
                &#39;contains a single sample.&#39;,
            )
        n_features: int
        n_samples: int
        n_samples, n_features = X.shape

        self._n_samples: int = n_samples
        self._n_features: int = n_features

        X: np.ndarray = _standard_scaler(X)
        if self._metric == &#39;correlation&#39;:
            corr = _correlation(X)
            matrix_ = np.abs(corr)
        elif self._metric == &#39;GY&#39; and self._knn_estimate:
            matrix_ = self._nonlinear_GY_knn(X)
        else:  # &#39;NMI&#39;, &#39;JSD&#39;, &#39;GY
            matrix_ = self._nonlinear_correlation(X)
        self.matrix_: np.ndarray = np.clip(matrix_, a_min=0, a_max=1)

    @fit.register
    def _(self, X: str, y=None) -&gt; None:
        &#34;&#34;&#34;Dispatched for online=True with string.&#34;&#34;&#34;
        self._reset()

        corr: np.ndarray
        matrix_: np.ndarray
        # parse data
        if not self._online:
            raise ValueError(&#39;Mode online=False reuqires X:np.ndarray.&#39;)

        if self._metric == &#39;correlation&#39;:
            corr = self._online_correlation(X)
            matrix_ = np.abs(corr)
        else:
            raise ValueError(
                &#39;Mode online=True is only implemented for correlation.&#39;,
            )

        self.matrix_: np.ndarray = np.clip(matrix_, a_min=0, a_max=1)

    @beartype
    def _reset(self) -&gt; None:
        &#34;&#34;&#34;Reset internal data-dependent state of correlation.&#34;&#34;&#34;
        if hasattr(self, &#39;_filename&#39;):  # noqa: WPS421
            del self._filename  # noqa: WPS420
        if hasattr(self, &#39;matrix_&#39;):  # noqa: WPS421
            del self.matrix_  # noqa: WPS420

    @beartype
    def _nonlinear_correlation(self, X: Float2DArray) -&gt; FloatMatrix:
        &#34;&#34;&#34;Return the nonlinear correlation.&#34;&#34;&#34;
        calc_nl_corr: Callable = {
            &#39;NMI&#39;: self._nmi,
            &#39;GY&#39;: self._gy,
            &#39;JSD&#39;: self._jsd,
        }[self._metric]

        nl_corr: FloatMatrix = np.empty(  # noqa: WPS317
            (self._n_features, self._n_features), dtype=self._dtype,
        )
        for idx_i in range(self._n_features):
            xi = X[:, idx_i]
            nl_corr[idx_i, idx_i] = 1
            for idx_j in range(idx_i + 1, self._n_features):
                xj = X[:, idx_j]
                nl_corr_ij = calc_nl_corr(*_estimate_densities(xi, xj))
                nl_corr[idx_i, idx_j] = nl_corr_ij
                nl_corr[idx_j, idx_i] = nl_corr_ij

        return nl_corr

    @beartype
    def _nonlinear_GY_knn(self, X: Float2DArray) -&gt; FloatMatrix:
        &#34;&#34;&#34;Return the nonlinear correlation matrix based on Gel&#39;fand-Yaglom
        based on a reliable knn-estimate of the mutual information.&#34;&#34;&#34;
        nl_knn_corr = _knn_mutual_information(X, self._n_features)
        return np.sqrt(
            1 - np.exp(-2 * nl_knn_corr),
        )

    @beartype
    def _gy(
        self,
        pij: Float2DArray,
        pipj: Float2DArray,
        pi: Float1DArray,
        pj: Float1DArray,
    ) -&gt; float:
        &#34;&#34;&#34;Return the dissimilarity based on Gel&#39;fand-Yaglom.&#34;&#34;&#34;
        mutual_info: float = _kullback(pij, pipj)
        return np.sqrt(
            1 - np.exp(-2 * mutual_info),
        )

    @beartype
    def _jsd(
        self,
        pij: Float2DArray,
        pipj: Float2DArray,
        pi: Float1DArray,
        pj: Float1DArray,
    ) -&gt; float:
        &#34;&#34;&#34;Return the Jensen-Shannon based dissimilarity.&#34;&#34;&#34;
        return jensenshannon(
            pij.flatten(),
            pipj.flatten(),
            base=2,
        )

    @beartype
    def _nmi(
        self,
        pij: Float2DArray,
        pipj: Float2DArray,
        pi: Float1DArray,
        pj: Float1DArray,
    ) -&gt; float:
        &#34;&#34;&#34;Return the Jensen-Shannon based dissimilarity.&#34;&#34;&#34;
        mutual_info: float = _kullback(pij, pipj)
        normalization: float = self._normalization(pi, pj, pij)
        return mutual_info / normalization

    @beartype
    def _normalization(
        self, pi: np.ndarray, pj: np.ndarray, pij: np.ndarray,
    ) -&gt; float:
        &#34;&#34;&#34;Calculate the normalization factor for the MI matrix.&#34;&#34;&#34;
        method: str = self._normalize_method
        if method == &#39;joint&#39;:
            return _entropy(pij)

        func: Callable = {
            &#39;geometric&#39;: lambda arr: np.sqrt(np.prod(arr)),
            &#39;arithmetic&#39;: np.mean,
            &#39;min&#39;: np.min,
            &#39;max&#39;: np.max,
        }[method]
        return func([_entropy(pi), _entropy(pj)])

    @beartype
    def _online_correlation(self, X: str) -&gt; FloatMatrix:
        &#34;&#34;&#34;Calculate correlation on the fly.&#34;&#34;&#34;
        self._filename: str = X
        self._n_features: int = len(next(self._data_gen()))
        # parse mean, std and corr
        return self._welford_correlation()

    @beartype
    def _data_gen(
        self, comments: Union[str, Tuple[str, ...]] = (&#39;#&#39;, &#39;@&#39;),
    ) -&gt; Generator[Float1DArray, None, None]:
        &#34;&#34;&#34;Return all non comment lines as generator.&#34;&#34;&#34;
        with open(self._filename) as file_obj:
            for line in file_obj:
                if line.startswith(comments):
                    continue
                yield np.array(line.split()).astype(self._dtype)

    @beartype
    def _welford_correlation(self) -&gt; FloatMatrix:
        &#34;&#34;&#34;Calculate the correlation via online Welford algorithm.

        Welford algorithm, generalized to correlation. Taken from:
        Donald E. Knuth (1998). The Art of Computer Programming, volume 2:
        Seminumerical Algorithms, 3rd edn., p. 232. Boston: Addison-Wesley.

        &#34;&#34;&#34;
        n: int = 0
        mean: np.ndarray = np.zeros(self._n_features, dtype=self._dtype)
        corr: np.ndarray = np.zeros(  # noqa: WPS317
            (self._n_features, self._n_features), dtype=self._dtype,
        )

        for x in self._data_gen():
            n += 1
            dx: np.ndarray = x - mean
            mean = mean + dx / n
            corr = corr + dx.reshape(-1, 1) * (
                x - mean
            ).reshape(1, -1)

        self._n_samples = n
        if n &lt; 2:
            return np.full_like(corr, np.nan)

        std = np.sqrt(np.diag(corr) / (n - 1))
        return corr / (n - 1) / (
            std.reshape(-1, 1) * std.reshape(1, -1)
        )</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="mosaic.Similarity.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the correlation/nmi distance matrix.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_samples, n_features)</code> or <code>str if online=True</code></dt>
<dd>Training data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>Ignored</code></dt>
<dd>Not used, present for scikit API consistency by convention.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@singledispatchmethod
@beartype
def fit(
    self,
    X: Union[FloatMax2DArray, str],
    y: Optional[ArrayLikeFloat] = None,
) -&gt; None:
    &#34;&#34;&#34;Compute the correlation/nmi distance matrix.

    Parameters
    ----------
    X : ndarray of shape (n_samples, n_features) or str if online=True
        Training data.

    y : Ignored
        Not used, present for scikit API consistency by convention.

    &#34;&#34;&#34;
    raise NotImplementedError(&#39;Fatal error, this should never be reached.&#39;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="mosaic.UMAPSimilarity"><code class="flex name class">
<span>class <span class="ident">UMAPSimilarity</span></span>
<span>(</span><span>*, densmap=True, n_neighbors=None, n_components=2)</span>
</code></dt>
<dd>
<div class="desc"><p>Class for embedding similarity matrix with UMAP.</p>
<p>For more details on the parameters check the UMAP documentation.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>densmap</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>If True the density-augmented objective of densMAP is used for
optimization. There the local densities are encouraged to be correlated
with those in the original space.</dd>
<dt><strong><code>n_neighbors</code></strong> :&ensp;<code>int</code>, default=<code>None</code></dt>
<dd>Size of nearest neighbors used for manifold estimation in UMAP.
If <code>None</code> uses square root of the number of features.</dd>
<dt><strong><code>n_components</code></strong> :&ensp;<code>int</code>, default=<code>2</code></dt>
<dd>Dimensionality of the local embedding.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>matrix_</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_features, n_features)</code></dt>
<dd>Normalized pairwise distance matrix of the UMAP embedding.</dd>
<dt><strong><code>embedding_</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_features, n_components)</code></dt>
<dd>Coordinates of features in UMAP embedding.</dd>
<dt><strong><code>n_neighbors_</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of used neighbors.</dd>
</dl>
<p>Initialize UMAPSimilarity class.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class UMAPSimilarity:  # noqa: WPS214
    r&#34;&#34;&#34;Class for embedding similarity matrix with UMAP.

    For more details on the parameters check the UMAP documentation.

    Parameters
    ----------
    densmap : bool, default=True
        If True the density-augmented objective of densMAP is used for
        optimization. There the local densities are encouraged to be correlated
        with those in the original space.

    n_neighbors: int, default=None
        Size of nearest neighbors used for manifold estimation in UMAP.
        If `None` uses square root of the number of features.

    n_components: int, default=2
        Dimensionality of the local embedding.

    Attributes
    ----------
    matrix_ : ndarray of shape (n_features, n_features)
        Normalized pairwise distance matrix of the UMAP embedding.

    embedding_ : ndarray of shape (n_features, n_components)
        Coordinates of features in UMAP embedding.

    n_neighbors_ : int
        Number of used neighbors.

    &#34;&#34;&#34;

    _default_n_components: PositiveInt = 2

    @beartype
    def __init__(
        self,
        *,
        densmap: bool = True,
        n_neighbors: Optional[PositiveInt] = None,
        n_components: PositiveInt = _default_n_components,
    ):
        &#34;&#34;&#34;Initialize UMAPSimilarity class.&#34;&#34;&#34;
        self._densmap: bool = densmap
        self._n_neighbors: Optional[PositiveInt] = n_neighbors
        self._n_components: PositiveInt = n_components

    @beartype
    def fit(
        self,
        X: Float2DArray,
        y: Optional[ArrayLikeFloat] = None,
    ) -&gt; None:
        &#34;&#34;&#34;Fit similarity matrix into UMAP embedding.&#34;&#34;&#34;
        self._reset()

        # parse data
        self._n_features: int = X.shape[0]

        # parse n_neighbors
        if self._n_neighbors is None:
            self._n_neighbors = np.ceil(np.sqrt(self._n_features)).astype(int)
        elif self._n_neighbors &gt;= len(X):
            raise ValueError(
                &#39;The number of nearest neighbors must be smaller than the &#39;
                &#39;number of features.&#39;,
            )
        self.n_neighbors_: PositiveInt = self._n_neighbors

        reducer = umap.UMAP(
            n_neighbors=self._n_neighbors,
            densmap=self._densmap,
            n_components=self._n_components,
            metric=&#39;precomputed&#39;,
        )

        # run UMAP with dissimalirty matrix
        with warnings.catch_warnings():
            warnings.filterwarnings(
                &#39;ignore&#39;,
                message=&#39;using precomputed metric&#39;,
            )
            embedding: np.ndarray = reducer.fit_transform(1 - X)
        matrix_: np.ndarray = _calc_distance_matrix(embedding)

        self.embedding_: np.ndarray = embedding
        self.matrix_: np.ndarray = 1 - matrix_ / np.nanmax(matrix_)

    @beartype
    def _reset(self) -&gt; None:
        &#34;&#34;&#34;Reset internal data-dependent state of correlation.&#34;&#34;&#34;
        if hasattr(self, &#39;matrix_&#39;):  # noqa: WPS421
            del self.matrix_  # noqa: WPS420</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="mosaic.UMAPSimilarity.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Fit similarity matrix into UMAP embedding.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype
def fit(
    self,
    X: Float2DArray,
    y: Optional[ArrayLikeFloat] = None,
) -&gt; None:
    &#34;&#34;&#34;Fit similarity matrix into UMAP embedding.&#34;&#34;&#34;
    self._reset()

    # parse data
    self._n_features: int = X.shape[0]

    # parse n_neighbors
    if self._n_neighbors is None:
        self._n_neighbors = np.ceil(np.sqrt(self._n_features)).astype(int)
    elif self._n_neighbors &gt;= len(X):
        raise ValueError(
            &#39;The number of nearest neighbors must be smaller than the &#39;
            &#39;number of features.&#39;,
        )
    self.n_neighbors_: PositiveInt = self._n_neighbors

    reducer = umap.UMAP(
        n_neighbors=self._n_neighbors,
        densmap=self._densmap,
        n_components=self._n_components,
        metric=&#39;precomputed&#39;,
    )

    # run UMAP with dissimalirty matrix
    with warnings.catch_warnings():
        warnings.filterwarnings(
            &#39;ignore&#39;,
            message=&#39;using precomputed metric&#39;,
        )
        embedding: np.ndarray = reducer.fit_transform(1 - X)
    matrix_: np.ndarray = _calc_distance_matrix(embedding)

    self.embedding_: np.ndarray = embedding
    self.matrix_: np.ndarray = 1 - matrix_ / np.nanmax(matrix_)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="MoSAIC home" href="https://moldyn.github.io/MoSAIC">
<img src="logo_large_light.svg" style="width: 200px;" class="lightmode" alt="">
</a>
</header>
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#molecular-systems-automated-identification-of-cooperativity">Molecular Systems Automated Identification of Cooperativity</a><ul>
<li><a href="#features">Features</a></li>
<li><a href="#installation">Installation</a><ul>
<li><a href="#shell-completion">Shell Completion</a></li>
</ul>
</li>
<li><a href="#usage">Usage</a><ul>
<li><a href="#ci-usage-directly-from-the-command-line">CI - Usage Directly from the Command Line</a></li>
<li><a href="#module-inside-a-python-script">Module - Inside a Python Script</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<ul id="index">
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="mosaic.clustering" href="clustering.html">mosaic.clustering</a></code></li>
<li><code><a title="mosaic.similarity" href="similarity.html">mosaic.similarity</a></code></li>
<li><code><a title="mosaic.tools" href="tools.html">mosaic.tools</a></code></li>
<li><code><a title="mosaic.umap_similarity" href="umap_similarity.html">mosaic.umap_similarity</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="mosaic.load_clusters" href="#mosaic.load_clusters">load_clusters</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="mosaic.Clustering" href="#mosaic.Clustering">Clustering</a></code></h4>
<ul class="">
<li><code><a title="mosaic.Clustering.fit" href="#mosaic.Clustering.fit">fit</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mosaic.Similarity" href="#mosaic.Similarity">Similarity</a></code></h4>
<ul class="">
<li><code><a title="mosaic.Similarity.fit" href="#mosaic.Similarity.fit">fit</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mosaic.UMAPSimilarity" href="#mosaic.UMAPSimilarity">UMAPSimilarity</a></code></h4>
<ul class="">
<li><code><a title="mosaic.UMAPSimilarity.fit" href="#mosaic.UMAPSimilarity.fit">fit</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<a href="https://github.com/moldyn/MoSAIC" class="github-corner" aria-label="View source on GitHub">
<svg width="80" height="80" viewBox="0 0 250 250" style="fill:#4d4f53; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true">
<path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
<path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
<path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path>
</svg>
</a>
<style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>